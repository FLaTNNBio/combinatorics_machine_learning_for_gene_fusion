{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mPnz-PYM1z"
      },
      "source": [
        "#Combinatorics and Machine Learning for Gene Fusion\n",
        "MGE and MML code for experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asmlo-HGYU3z"
      },
      "source": [
        "Prepocessing of the data to train the GCN for fusion graph classification.\n",
        "Each fingerprints is converted a de Bruijn graph and after convertend in pytorch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJwZNduKwe00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to read and process the file\n",
        "def process_file(filename, label):\n",
        "    with open(filename, 'r') as file:\n",
        "        data = file.read()\n",
        "    entries = data.split(\"', '\")\n",
        "    entries = [entry.strip(\" '\") for entry in entries]\n",
        "    df = pd.DataFrame(entries, columns=[\"Entry\"])\n",
        "    df['Label'] = label\n",
        "    return df\n",
        "\n",
        "# Function to extract numeric data from entries\n",
        "def extract_numbers(df):\n",
        "    def extract_numeric_part(entry):\n",
        "        match = re.search(r'\\|([\\d\\s|]+)', entry)\n",
        "        if match:\n",
        "            return ' '.join(match.group(1).strip().split('|')).strip()\n",
        "        return \"\"\n",
        "    numeric_data = df['Entry'].apply(extract_numeric_part)\n",
        "    numeric_data = numeric_data.apply(lambda x: list(map(int, x.split())) if x else [])\n",
        "    return pd.DataFrame({'Numbers': numeric_data, 'Label': df['Label']})\n",
        "\n",
        "# Function to generate kmers from a sequence\n",
        "def get_kmer(sequence, k=4):\n",
        "    if len(sequence) < k:\n",
        "        return []  # Return an empty list if the sequence is too short\n",
        "    kmers = []\n",
        "    sequence = ''.join(map(str, sequence))  # Convert sequence list to string\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        kmers.append(sequence[i:i + k])\n",
        "    return kmers\n",
        "\n",
        "# Function to generate De Bruijn graph edges from kmers\n",
        "def get_debruijn_edges(kmers):\n",
        "    edges = set()\n",
        "    for k1 in kmers:\n",
        "        for k2 in kmers:\n",
        "            if k1 != k2 and k1[1:] == k2[:-1]:\n",
        "                edges.add((k1, k2))\n",
        "    return edges\n",
        "\n",
        "# Function to create a NetworkX graph from a sequence\n",
        "def sequence_to_nx_graph(sequence, k=4):\n",
        "    kmers = get_kmer(sequence, k)\n",
        "    if not kmers:\n",
        "        return nx.DiGraph()  # Return an empty graph if no k-mers were generated\n",
        "    edges = get_debruijn_edges(kmers)\n",
        "    G = nx.DiGraph()\n",
        "    for kmer in kmers:\n",
        "        G.add_node(kmer, kmer_feature=kmer)\n",
        "    G.add_edges_from(edges)\n",
        "    return G\n",
        "\n",
        "def convert_nx_to_torch_geo(G, label):\n",
        "    kmer_list = list(G.nodes())\n",
        "    if not kmer_list:\n",
        "        return None  # Return None or handle appropriately for graphs with no nodes\n",
        "\n",
        "    # Assuming each k-mer is a string of numbers, and we convert each character to an integer\n",
        "    max_kmer_len = max(len(kmer) for kmer in kmer_list) if kmer_list else 0\n",
        "    x = torch.zeros((len(kmer_list), max_kmer_len), dtype=torch.float)\n",
        "    for i, kmer in enumerate(kmer_list):\n",
        "        int_kmer = [int(char) for char in kmer]  # Convert each character to an integer\n",
        "        x[i, :len(int_kmer)] = torch.tensor(int_kmer, dtype=torch.float)\n",
        "\n",
        "    # Prepare edge indices\n",
        "    if len(G.edges()) == 0:\n",
        "        print(f\"Warning: Graph with label {label} has no edges.\")\n",
        "        return None  # Skip graphs with no edges\n",
        "\n",
        "    kmer_index = {kmer: idx for idx, kmer in enumerate(kmer_list)}\n",
        "    edge_index = torch.tensor([[kmer_index[u], kmer_index[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # Prepare label\n",
        "    y = torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "    # Create PyTorch Geometric Data object\n",
        "    data = Data(x=x, edge_index=edge_index, y=y, num_nodes=len(G.nodes()))\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# Pre-process checks to filter out invalid sequences\n",
        "def filter_valid_rows(df, k):\n",
        "    valid_rows = []\n",
        "    for index, row in df.iterrows():\n",
        "        sequence = row['Numbers']\n",
        "        if len(sequence) >= k:\n",
        "            valid_rows.append(row)\n",
        "        else:\n",
        "            print(f\"Skipping row {index} due to insufficient sequence length.\", sequence)\n",
        "    return pd.DataFrame(valid_rows)\n",
        "\n",
        "\n",
        "# Load and process data\n",
        "filename_fuse = '/content/fingerprint_fuse.txt'\n",
        "filename_no_fuse = '/content/fingerprint_no_fuse.txt'  # Change this to the actual filename\n",
        "df_no_fuse = process_file(filename_no_fuse, 0)\n",
        "df_fuse = process_file(filename_fuse, 1)\n",
        "df_no_fuse = extract_numbers(df_no_fuse)\n",
        "df_fuse = extract_numbers(df_fuse)\n",
        "\n",
        "print(df_no_fuse.head())\n",
        "print(df_fuse.head())\n",
        "\n",
        "\n",
        "# Filter valid rows based on k-mer size\n",
        "k = 4\n",
        "df_no_fuse = filter_valid_rows(df_no_fuse, k)\n",
        "df_fuse = filter_valid_rows(df_fuse, k)\n",
        "\n",
        "# Extract numbers and convert each row to a graph, then to a PyTorch Geometric Data object\n",
        "data_list_no_fuse = []\n",
        "data_list_fuse = []\n",
        "\n",
        "for index, row in df_no_fuse.iterrows():\n",
        "    sequence = row['Numbers']\n",
        "    graph = sequence_to_nx_graph(sequence, k)\n",
        "    data_object = convert_nx_to_torch_geo(graph, label=row['Label'])\n",
        "    if data_object is not None:\n",
        "        data_list_no_fuse.append(data_object)\n",
        "\n",
        "for index, row in df_fuse.iterrows():\n",
        "    sequence = row['Numbers']\n",
        "    graph = sequence_to_nx_graph(sequence, k)\n",
        "    data_object = convert_nx_to_torch_geo(graph, label=row['Label'])\n",
        "    if data_object is not None:\n",
        "        data_list_fuse.append(data_object)\n",
        "\n",
        "# Combine the two lists\n",
        "all_data = data_list_no_fuse + data_list_fuse\n",
        "\n",
        "# Verify all graphs have nodes and edges\n",
        "filtered_data_list = [data for data in all_data if data is not None and data.x.size(0) > 0 and data.edge_index.size(1) > 0]\n",
        "print(f\"Number of valid graphs after filtering: {len(filtered_data_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZwD2_ZdaZDN"
      },
      "source": [
        "GCN definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U7XWcR9jZCK"
      },
      "outputs": [],
      "source": [
        "# Define the GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)  # Aggregates node embeddings into graph embeddings\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Set the parameters\n",
        "in_channels = 4       # Each node has 4 features\n",
        "hidden_channels = 16  # Number of hidden units\n",
        "out_channels = 2      # Binary classification (2 classes)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GCN(in_channels, hidden_channels, out_channels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3B-dapabiu"
      },
      "source": [
        "Training and testing the model on a selected number of balanced datasets based on minority class balancing.\n",
        "\n",
        "The minority class determines how many elements from the majority class need to be randomly selected to create a balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUyMX_NQZSZD"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv torch\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Separate the minority and dominant class samples\n",
        "minority_class = [data for data in filtered_data_list if data.y.item() == 0]\n",
        "dominant_class = [data for data in filtered_data_list if data.y.item() == 1]\n",
        "\n",
        "# Define the number of balanced datasets you want to create\n",
        "num_datasets = 5\n",
        "\n",
        "# Initialize a list to store the balanced datasets\n",
        "balanced_datasets = []\n",
        "\n",
        "# Seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Create multiple balanced datasets\n",
        "for _ in range(num_datasets):\n",
        "    # Randomly sample from the dominant class\n",
        "    sampled_dominant_class = random.sample(dominant_class, len(minority_class))\n",
        "\n",
        "    # Combine the minority class with the sampled dominant class\n",
        "    balanced_dataset = minority_class + sampled_dominant_class\n",
        "\n",
        "    # Shuffle the combined dataset\n",
        "    random.shuffle(balanced_dataset)\n",
        "\n",
        "    # Add the balanced dataset to the list\n",
        "    balanced_datasets.append(balanced_dataset)\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model\n",
        "in_channels = 4  # Example input feature size, adjust as necessary\n",
        "hidden_channels = 16\n",
        "out_channels = 2  # For binary classification\n",
        "model = GCN(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels)\n",
        "\n",
        "# Define the optimizer and loss criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adjusted learning rate\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "        elif val_loss > self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Initialize performance tracking\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "roc_aucs = []\n",
        "\n",
        "# Process each balanced dataset\n",
        "for i, selected_dataset in enumerate(balanced_datasets):\n",
        "    print(f\"\\nProcessing Balanced Dataset {i+1}/{num_datasets}\")\n",
        "\n",
        "    # Perform a train-val-test split on the selected balanced dataset\n",
        "    train_data, temp_data = train_test_split(\n",
        "        selected_dataset,\n",
        "        test_size=0.4,  # 40% goes to temp set which will be split into val and test\n",
        "        random_state=42,\n",
        "        stratify=[data.y.item() for data in selected_dataset]\n",
        "    )\n",
        "\n",
        "    val_data, test_data = train_test_split(\n",
        "        temp_data,\n",
        "        test_size=0.5,  # Split temp set equally for val and test\n",
        "        random_state=42,\n",
        "        stratify=[data.y.item() for data in temp_data]\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders for training, validation, and testing\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop with detailed reporting\n",
        "    def train():\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            if data.y.size(0) == 0:\n",
        "                print(\"Empty batch detected\")\n",
        "                continue  # Skip processing if the batch is empty\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            total_samples += data.y.size(0)\n",
        "            correct += (out.argmax(dim=1) == data.y).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        accuracy = correct / total_samples if total_samples > 0 else 0\n",
        "        print(f\"Training - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Total Batches: {num_batches}, Total Samples: {total_samples}\")\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    # Testing loop with ROC AUC calculation\n",
        "    def test(loader, phase='Testing'):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                if data.y.size(0) == 0:\n",
        "                    print(\"Empty batch detected\")\n",
        "                    continue  # Skip processing if the batch is empty\n",
        "\n",
        "                out = model(data)\n",
        "                prob = F.softmax(out, dim=1)[:, 1]  # Probability of class 1\n",
        "                pred = out.argmax(dim=1)\n",
        "\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "\n",
        "                all_predictions.extend(pred.tolist())\n",
        "                all_labels.extend(data.y.tolist())\n",
        "                all_probabilities.extend(prob.tolist())\n",
        "\n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "        roc_auc = roc_auc_score(all_labels, all_probabilities) if len(set(all_labels)) > 1 else float('nan')  # Handle single class case\n",
        "        print(f\"{phase} - Total Correct: {correct}, Total Samples: {total}, Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "        # Detailed prediction report\n",
        "        class_0_count = all_predictions.count(0)\n",
        "        class_1_count = all_predictions.count(1)\n",
        "        print(f\"Prediction Distribution - Class 0: {class_0_count}, Class 1: {class_1_count}\")\n",
        "\n",
        "        return accuracy, roc_auc\n",
        "\n",
        "    # Train and evaluate the model with detailed reporting\n",
        "    early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        print(f\"Epoch {epoch+1}/{100}\")\n",
        "        loss, train_acc = train()\n",
        "        val_acc, _ = test(val_loader, phase='Validation')\n",
        "        scheduler.step(loss)  # Adjust learning rate based on validation loss\n",
        "        print(f\"Epoch {epoch+1} Summary: Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\\n\")\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopping(val_acc)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # Evaluate on the test set after training\n",
        "    test_acc, roc_auc = test(test_loader, phase='Test')\n",
        "\n",
        "    # Record the performance for each dataset\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "# Calculate and print the mean performance across all datasets\n",
        "mean_train_accuracy = np.mean(train_accuracies)\n",
        "mean_val_accuracy = np.mean(val_accuracies)\n",
        "mean_test_accuracy = np.mean(test_accuracies)\n",
        "mean_roc_auc = np.nanmean(roc_aucs)  # Use nanmean to ignore NaNs from single class scenarios\n",
        "\n",
        "print(\"\\nOverall Performance Across All Datasets\")\n",
        "print(f\"Mean Training Accuracy: {mean_train_accuracy:.4f}\")\n",
        "print(f\"Mean Validation Accuracy: {mean_val_accuracy:.4f}\")\n",
        "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}\")\n",
        "print(f\"Mean ROC AUC: {mean_roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0YU__1RNd4"
      },
      "source": [
        "\n",
        "Training and testing using scattefold techniques on a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTUvujObw7MO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "# Define your GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model\n",
        "in_channels = 4  # Adjust based on your dataset\n",
        "hidden_channels = 16\n",
        "out_channels = 2  # For binary classification\n",
        "model = GCN(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels)\n",
        "\n",
        "# Define the optimizer and loss criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Split the perfectly balanced dataset into train, validation, and test sets\n",
        "train_data, temp_data = train_test_split(\n",
        "    filtered_data_list,\n",
        "    test_size=0.4,  # 40% goes to temp set which will be split into val and test\n",
        "    random_state=42,\n",
        "    stratify=[data.y.item() for data in filtered_data_list]\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,  # Split temp set equally for val and test\n",
        "    random_state=42,\n",
        "    stratify=[data.y.item() for data in temp_data]\n",
        ")\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.01):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_score\n",
        "            self.counter = 0\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        if data.y.size(0) == 0:\n",
        "            print(\"Empty batch detected\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        total_samples += data.y.size(0)\n",
        "        correct += (out.argmax(dim=1) == data.y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "    accuracy = correct / total_samples if total_samples > 0 else 0\n",
        "    print(f\"Training - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Total Batches: {num_batches}, Total Samples: {total_samples}\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Testing function with additional metrics calculation\n",
        "def test(loader, phase='Testing'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            if data.y.size(0) == 0:\n",
        "                print(\"Empty batch detected\")\n",
        "                continue\n",
        "\n",
        "            out = model(data)\n",
        "            prob = F.softmax(out, dim=1)[:, 1]  # Probability of class 1\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total += data.y.size(0)\n",
        "\n",
        "            all_predictions.extend(pred.tolist())\n",
        "            all_labels.extend(data.y.tolist())\n",
        "            all_probabilities.extend(prob.tolist())\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    roc_auc = roc_auc_score(all_labels, all_probabilities) if len(set(all_labels)) > 1 else float('nan')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
        "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
        "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"{phase} - Total Correct: {correct}, Total Samples: {total}, Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"{phase} - F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "    print(f\"{phase} - Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    return accuracy, roc_auc, f1, precision, recall, cm\n",
        "\n",
        "# Track performances\n",
        "performance_history = {\n",
        "    \"epoch\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"train_accuracy\": [],\n",
        "    \"val_accuracy\": [],\n",
        "    \"val_roc_auc\": [],\n",
        "    \"val_f1_score\": [],\n",
        "    \"val_precision\": [],\n",
        "    \"val_recall\": [],\n",
        "}\n",
        "\n",
        "# Train and evaluate the model with early stopping\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    print(f\"Epoch {epoch+1}/{100}\")\n",
        "    loss, train_acc = train()\n",
        "    val_acc, val_roc_auc, val_f1, val_precision, val_recall, _ = test(val_loader, phase='Validation')\n",
        "    print(f\"Epoch {epoch+1} Summary: Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val ROC AUC: {val_roc_auc:.4f}\\n\")\n",
        "\n",
        "    # Record performance metrics\n",
        "    performance_history[\"epoch\"].append(epoch + 1)\n",
        "    performance_history[\"train_loss\"].append(loss)\n",
        "    performance_history[\"train_accuracy\"].append(train_acc)\n",
        "    performance_history[\"val_accuracy\"].append(val_acc)\n",
        "    performance_history[\"val_roc_auc\"].append(val_roc_auc)\n",
        "    performance_history[\"val_f1_score\"].append(val_f1)\n",
        "    performance_history[\"val_precision\"].append(val_precision)\n",
        "    performance_history[\"val_recall\"].append(val_recall)\n",
        "\n",
        "    # Check early stopping\n",
        "    early_stopping(val_roc_auc)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Evaluate on the test set after training\n",
        "test_acc, roc_auc, f1, precision, recall, cm = test(test_loader, phase='Test')\n",
        "\n",
        "# Save the final test performance to the history\n",
        "performance_history[\"test_accuracy\"] = test_acc\n",
        "performance_history[\"test_roc_auc\"] = roc_auc\n",
        "performance_history[\"test_f1_score\"] = f1\n",
        "performance_history[\"test_precision\"] = precision\n",
        "performance_history[\"test_recall\"] = recall\n",
        "performance_history[\"confusion_matrix\"] = cm\n",
        "\n",
        "# Save performance metrics to a file\n",
        "with open('./performance_metrics.txt', 'w') as f:\n",
        "    f.write(\"Epoch,Train Loss,Train Accuracy,Val Accuracy,Val ROC AUC,Val F1 Score,Val Precision,Val Recall\\n\")\n",
        "    for i in range(len(performance_history[\"epoch\"])):\n",
        "        f.write(f\"{performance_history['epoch'][i]},\"\n",
        "                f\"{performance_history['train_loss'][i]:.4f},\"\n",
        "                f\"{performance_history['train_accuracy'][i]:.4f},\"\n",
        "                f\"{performance_history['val_accuracy'][i]:.4f},\"\n",
        "                f\"{performance_history['val_roc_auc'][i]:.4f},\"\n",
        "                f\"{performance_history['val_f1_score'][i]:.4f},\"\n",
        "                f\"{performance_history['val_precision'][i]:.4f},\"\n",
        "                f\"{performance_history['val_recall'][i]:.4f}\\n\")\n",
        "    f.write(\"\\nFinal Test Performance\\n\")\n",
        "    f.write(f\"Test Accuracy: {performance_history['test_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Test ROC AUC: {performance_history['test_roc_auc']:.4f}\\n\")\n",
        "    f.write(f\"Test F1 Score: {performance_history['test_f1_score']:.4f}\\n\")\n",
        "    f.write(f\"Test Precision: {performance_history['test_precision']:.4f}\\n\")\n",
        "    f.write(f\"Test Recall: {performance_history['test_recall']:.4f}\\n\")\n",
        "    f.write(f\"Confusion Matrix:\\n{performance_history['confusion_matrix']}\\n\")\n",
        "\n",
        "print(\"\\nFinal Performance\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qccNusmQGsL"
      },
      "source": [
        "#Hypergraph experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qVWeITYboQs"
      },
      "source": [
        "Construction of the De Bruij Hypergraph from a pool of fingerprints and convertion to a pytoch Data element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9LU2E9SQI1o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import gc  # Import garbage collection module\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to read and process the file\n",
        "def process_file(filename, label):\n",
        "    with open(filename, 'r') as file:\n",
        "        data = file.read()\n",
        "    entries = data.split(\"', '\")\n",
        "    entries = [entry.strip(\" '\") for entry in entries]\n",
        "    df = pd.DataFrame(entries, columns=[\"Entry\"])\n",
        "    df['Label'] = label\n",
        "    return df\n",
        "\n",
        "# Function to extract numeric data from entries\n",
        "def extract_numbers(df):\n",
        "    def extract_numeric_part(entry):\n",
        "        match = re.search(r'\\|([\\d\\s|]+)', entry)\n",
        "        if match:\n",
        "            return ' '.join(match.group(1).strip().split('|')).strip()\n",
        "        return \"\"\n",
        "    numeric_data = df['Entry'].apply(extract_numeric_part)\n",
        "    numeric_data = numeric_data.apply(lambda x: list(map(int, x.split())) if x else [])\n",
        "    return pd.DataFrame({'Numbers': numeric_data, 'Label': df['Label']})\n",
        "\n",
        "# Function to create a De Bruijn hypergraph from fingerprints\n",
        "def create_de_bruijn_hypergraph(fingerprint, k=4):\n",
        "    G = nx.Graph()\n",
        "    nodes = []\n",
        "    k_fingers = []\n",
        "\n",
        "    # Generate k-fingers from the fingerprint\n",
        "    for i in range(len(fingerprint) - k + 1):\n",
        "        k_finger = tuple(fingerprint[i:i + k])\n",
        "        if k_finger not in nodes:\n",
        "            nodes.append(k_finger)\n",
        "        k_fingers.append(k_finger)\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    node_indices = {k_finger: idx for idx, k_finger in enumerate(nodes)}\n",
        "    for k_finger in nodes:\n",
        "        G.add_node(node_indices[k_finger], label=k_finger)\n",
        "\n",
        "    # Form hyperedges based on overlaps\n",
        "    for i in range(len(k_fingers) - 1):\n",
        "        if k_fingers[i][1:] == k_fingers[i + 1][:-1]:\n",
        "            G.add_edge(node_indices[k_fingers[i]], node_indices[k_fingers[i + 1]])\n",
        "\n",
        "    return G, node_indices\n",
        "\n",
        "# Convert k-mer tuples to numerical features\n",
        "def kmer_to_features(kmer, max_kmer_length):\n",
        "    # Convert k-mer into a fixed-length numerical vector\n",
        "    return torch.tensor([float(x) for x in kmer], dtype=torch.float)\n",
        "\n",
        "# Convert NetworkX graph to PyG Data\n",
        "def convert_hypergraph_to_data(G, nodes, label):\n",
        "    # Create edge index\n",
        "    edge_index_list = list(G.edges)\n",
        "\n",
        "    if len(edge_index_list) == 0:\n",
        "        print(f\"Warning: Hypergraph with label {label} has no edges.\")\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)  # Empty edge index\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_index_list).t().contiguous()\n",
        "\n",
        "    # Create node features from k-mers\n",
        "    x = torch.stack([kmer_to_features(G.nodes[i]['label'], len(G.nodes[i]['label'])) for i in range(len(nodes))])\n",
        "\n",
        "    y = torch.tensor([label], dtype=torch.long)\n",
        "    data = Data(x=x, edge_index=edge_index, y=y, num_nodes=len(G.nodes))\n",
        "    return data\n",
        "\n",
        "# Pre-process checks to filter out invalid sequences\n",
        "def filter_valid_rows(df, k):\n",
        "    valid_rows = []\n",
        "    for index, row in df.iterrows():\n",
        "        sequence = row['Numbers']\n",
        "        if len(sequence) >= k:\n",
        "            valid_rows.append(row)\n",
        "        else:\n",
        "            print(f\"Skipping row {index} due to insufficient sequence length.\", sequence)\n",
        "    return pd.DataFrame(valid_rows)\n",
        "\n",
        "# Load and process data\n",
        "filename_fuse = '/content/fingerprint_fuse.txt'\n",
        "filename_no_fuse = '/content/fingerprint_no_fuse.txt'  # Change this to the actual filename\n",
        "df_no_fuse = process_file(filename_no_fuse, 0)\n",
        "df_fuse = process_file(filename_fuse, 1)\n",
        "df_no_fuse = extract_numbers(df_no_fuse)\n",
        "df_fuse = extract_numbers(df_fuse)\n",
        "\n",
        "print(df_no_fuse.head())\n",
        "print(df_fuse.head())\n",
        "\n",
        "# Filter valid rows based on k-mer size\n",
        "k = 4  # Adjusted k according to the example\n",
        "df_no_fuse = filter_valid_rows(df_no_fuse, k)\n",
        "df_fuse = filter_valid_rows(df_fuse, k)\n",
        "\n",
        "# Convert each row to a hypergraph, then to a PyTorch Geometric Data object\n",
        "data_list_no_fuse = []\n",
        "data_list_fuse = []\n",
        "\n",
        "for index, row in df_no_fuse.iterrows():\n",
        "    fingerprint = row['Numbers']  # Use the sequence directly as fingerprints\n",
        "    graph, nodes = create_de_bruijn_hypergraph(fingerprint, k)\n",
        "    data_object = convert_hypergraph_to_data(graph, nodes, label=row['Label'])\n",
        "    if data_object is not None:\n",
        "        data_list_no_fuse.append(data_object)\n",
        "\n",
        "for index, row in df_fuse.iterrows():\n",
        "    fingerprint = row['Numbers']  # Use the sequence directly as fingerprints\n",
        "    graph, nodes = create_de_bruijn_hypergraph(fingerprint, k)\n",
        "    data_object = convert_hypergraph_to_data(graph, nodes, label=row['Label'])\n",
        "    if data_object is not None:\n",
        "        data_list_fuse.append(data_object)\n",
        "\n",
        "# Combine the two lists\n",
        "all_data = data_list_no_fuse + data_list_fuse\n",
        "\n",
        "# Verify all graphs have nodes and edges\n",
        "filtered_data_list = [data for data in all_data if data is not None and data.x.size(0) > 0 and (data.edge_index.size(1) > 0 or data.num_nodes == 1)]\n",
        "print(f\"Number of valid hypergraphs after filtering: {len(filtered_data_list)}\")\n",
        "\n",
        "# Print a summary of the transformed graphs\n",
        "for i, data in enumerate(filtered_data_list[:5]):  # Print only the first 5 for brevity\n",
        "    print(f\"Graph {i+1}:\")\n",
        "    print(f\"  Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"  Number of edges: {data.edge_index.size(1)}\")\n",
        "    print(f\"  Label: {data.y.item()}\")\n",
        "    print(\"  Node features (first 5 nodes):\")\n",
        "    print(data.x[:5])\n",
        "    print(\"  Edge index (first 5 edges):\")\n",
        "    print(data.edge_index[:, :5])\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitrW7QTb5Cf"
      },
      "source": [
        "Definition, Training, and Testing of a HyperGCN for Classifying the De Bruijn Hypergraph Previously Generated\n",
        "\n",
        "In this process, we define, train, and test a Hypergraph Convolutional Network (HyperGCN) to classify nodes or hyperedges in a De Bruijn hypergraph that was previously generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yY73u6agPa4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.loader import Data, DataLoader\n",
        "from torch_geometric.nn import HypergraphConv, global_mean_pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_data, temp_data = train_test_split(\n",
        "    filtered_data_list, test_size=0.4, random_state=42, stratify=[data.y.item() for data in filtered_data_list]\n",
        ")\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data, test_size=0.5, random_state=42, stratify=[data.y.item() for data in temp_data]\n",
        ")\n",
        "\n",
        "\n",
        "# Define the HGCN Model\n",
        "class HGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(HGCN, self).__init__()\n",
        "        self.conv1 = HypergraphConv(in_channels, hidden_channels)\n",
        "        self.conv2 = HypergraphConv(hidden_channels, hidden_channels)\n",
        "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = global_mean_pool(x, batch)  # Pool the node embeddings to obtain graph embeddings\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.01):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_score\n",
        "            self.counter = 0\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set in_channels to k to match k-mer size\n",
        "model = HGCN(in_channels=k, hidden_channels=16, out_channels=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    probabilities = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            prob = F.softmax(out, dim=1)[:, 1]  # Probability of the positive class\n",
        "            pred = out.argmax(dim=1)\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            labels.extend(data.y.cpu().numpy())\n",
        "            probabilities.extend(prob.cpu().numpy())\n",
        "            correct += pred.eq(data.y).sum().item()\n",
        "    return correct / len(loader.dataset), predictions, labels, probabilities\n",
        "\n",
        "# Main training loop with early stopping\n",
        "num_epochs = 100\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train()\n",
        "    train_acc, _, _, _ = evaluate(train_loader)\n",
        "    val_acc, _, _, _ = evaluate(val_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Check early stopping condition\n",
        "    early_stopping(val_acc)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_acc, predictions, labels, probabilities = evaluate(test_loader)\n",
        "\n",
        "# Calculate performance metrics\n",
        "f1 = f1_score(labels, predictions, average='binary')\n",
        "precision = precision_score(labels, predictions, average='binary')\n",
        "recall = recall_score(labels, predictions, average='binary')\n",
        "roc_auc = roc_auc_score(labels, probabilities)\n",
        "conf_matrix = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Save and print results\n",
        "performance_metrics = {\n",
        "    'Accuracy': test_acc,\n",
        "    'F1 Score': f1,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'ROC AUC': roc_auc,\n",
        "    'Confusion Matrix': conf_matrix\n",
        "}\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nFinal Test Performance Metrics:\")\n",
        "for metric, value in performance_metrics.items():\n",
        "    if metric != 'Confusion Matrix':\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}:\\n{value}\")\n",
        "\n",
        "\n",
        "  # Optionally, save the metrics to a file\n",
        "with open('./performance_metrics_hyper.txt', 'w') as f:\n",
        "    for metric, value in performance_metrics.items():\n",
        "        if metric != 'Confusion Matrix':\n",
        "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
        "        else:\n",
        "            f.write(f\"{metric}:\\n{value}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
